# -*- coding: utf-8 -*-
"""RainWater_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/AlishaVyas/RainWater-Prediction/blob/main/RainWater_prediction.ipynb

**Importing the dependencies**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import resample
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import pickle

"""**Data Collection and Processing**"""

# laod the dataset to a pandas dataframes
data = pd.read_csv("/content/Rainfall.csv")

print(type(data))

data.shape

data.head()

data.tail()

data["day"].unique()

print("Data Info:")
data.info()

data.columns

#remove extra spaces in all colums
data.columns = data.columns.str.strip()

data.columns

data.info()

data = data.drop(columns=["day"])

data.head()

#cheacking the number of missing values
data.isnull().sum()

print(data.isnull().sum())

data["winddirection"].unique()

# handel missing values
data["winddirection"] = data["winddirection"].fillna(data["winddirection"].mode()[0])
data["windspeed"] = data["windspeed"].fillna(data["windspeed"].median())

print(data.isnull().sum())

data["rainfall"].unique()

# converting the yes and no to 1 and 0 respectively
data["rainfall"] = data["rainfall"].map({"yes":1,"no":0})

data.head()

"""**Exploratory Data Analysis (EDA)**"""

data.shape

#setting plot style for all the plots
sns.set(style="whitegrid")

data.describe()

data.columns

plt.figure(figsize=(15,10))
for i, column in enumerate(['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', 'humidity',
       'cloud', 'sunshine','windspeed'],1):
  plt.subplot(3,3,i)
  sns.histplot(data[column], kde=True)
  plt.title(f"Distribution of {column}")

plt.tight_layout()
plt.show

plt.figure(figsize=(6,4))
sns.countplot(x="rainfall", data=data)
plt.title("Distribution of Rainfall")
plt.show()

# correlation matrix
plt.figure(figsize=(10,8))
sns.heatmap(data.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("correlation heatmap")
plt.show()

plt.figure(figsize=(15,10))
for i, column in enumerate(['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', 'humidity',
       'cloud', 'sunshine','windspeed'],1):
  plt.subplot(3,3,i)
  sns.boxplot(data[column])
  plt.title(f"Boxplot of {column}")

plt.tight_layout()
plt.show

"""**Data Preprocessing**"""

# drap highly correlated column
data = data.drop(columns=['maxtemp', 'temparature', 'mintemp'])

data.head()

print(data["rainfall"].value_counts())

# seperate majority and minority class
df_majority = data[data["rainfall"]==1]
df_minority = data[data["rainfall"]==0]

print(df_majority.shape)
print(df_minority.shape)

#downsample majority class to match minority count
df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)

df_majority_downsampled.shape

df_downsampled = pd.concat([df_majority_downsampled, df_minority])

df_downsampled.shape

df_downsampled.head()

#shuffle the final dataframe
df_downsampled = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)

df_downsampled.head()

df_downsampled["rainfall"].value_counts()

# split features and target as X and Y
x = df_downsampled.drop(columns=["rainfall"])
y = df_downsampled["rainfall"]

print(x)

print(y)

# splitting the data into training data and testing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)

"""**Model Traing**"""

rf_model = RandomForestClassifier(random_state=42)
param_grid_rf = {
    "n_estimators": [50,100,200],
    "max_features": ["sqrt", "log2"],
    "max_depth": [None, 10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}

# HYpertunning using GridSearchCV
grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)
grid_search_rf.fit(X_train, Y_train)

best_rf_model = grid_search_rf.best_estimator_
print("best parameters for random forest:", grid_search_rf.best_params_)

"""**Model Evaluation**

"""

cv_scores = cross_val_score(best_rf_model, X_train, Y_train, cv=5)
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation score:", np.mean(cv_scores))

# test set performance
y_pred = best_rf_model.predict(X_test)
accuracy = accuracy_score(Y_test, y_pred)
print("Test set accuracy:", accuracy)
print("Classification Report:\n", classification_report(Y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(Y_test, y_pred))

"""pridiction on unknown"""

X_train.columns

print(X_train.shape)  # Check if it has 7 or 8 columns
print(X_train.columns)  # Verify column names

input_data = (1015.9, 19.9, 95, 81, 0.0, 40.0, 13.7)
input_df = pd.DataFrame([input_data], columns=['pressure', 'dewpoint', 'humidity', 'cloud', 'sunshine',
       'winddirection', 'windspeed'])
prediction = best_rf_model.predict(input_df)

input_df

prediction = best_rf_model.predict(input_df)
print("prediction result:", "Rainfall" if prediction[0] == 1 else "No Rainfall" )

#save model and feature names to a pickle file
model_data ={"model": best_rf_model, "features_names": x.columns.tolist()}
with open("rainfall_prediction_model.pkl", "wb") as file:
  pickle.dump(model_data, file)

"""load the save model file and use it for prediction"""

import pickle
import pandas as pd

# load the trained model and features names from the pickle file
with open("rainfall_prediction_model.pkl", "rb") as file:
  model_data = pickle.load(file)

model = model_data["model"]
features_names = model_data["features_names"]

input_data = (1015.9, 19.9, 95, 81, 0.0, 40.0, 13.7)
input_df = pd.DataFrame([input_data], columns=features_names)

prediction = best_rf_model.predict(input_df)
print("prediction result:", "Rainfall" if prediction[0] == 1 else "No Rainfall" )

"""to try:
1. smote for class balancing
2. PCA for dimensionality reduction
3. simpler model like logistic regresion (with feature scaling)
4. model selection with hyper parameter tunning
"""